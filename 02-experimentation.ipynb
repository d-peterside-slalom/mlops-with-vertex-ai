{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae86f7",
   "metadata": {},
   "source": [
    "# 02 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/ai-platform-unified/docs/training/custom-training) to train a keras classifier to predict whether a given trip will result in a tip > 20%. The notebook covers the following tasks:\n",
    "1. Preprocess the data locally using Apache Beam.\n",
    "2. Train and test custom model locally using a Keras implementation.\n",
    "3. Submit a Dataflow job to preprocess the data at scale.\n",
    "4. Submit a custom training job to Vertex AI using a [pre-built container](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "5. Upload the trained model to Vertex AI.\n",
    "6. Track experiment parameters from [Vertex AI Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction).\n",
    "7. Submit a [hyperparameter tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to Vertex AI.\n",
    "\n",
    "We use [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) \n",
    "and [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) to  track, visualize, and compare ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5205917",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c6cfe",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4171d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:45:27.031678: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:45:28.037882: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:45:28.038024: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:45:28.038038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.11.0\n",
      "TensorFlow Transform: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hp_tuning\n",
    "\n",
    "from src.common import features, datasource_utils\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bc9635-2194-4d7f-b1e3-1e9ef2c2228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7f0315-133f-4d9b-a4f7-73a77a48b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.11.0\n",
      "TensorFlow Transform: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde361cf",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fce242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: g360-docai\n",
      "Region: us-west1\n",
      "Bucket name: vertex-mlops-chicago-taxi-bucket\n",
      "Bucket URI: gs://vertex-mlops-chicago-taxi-bucket\n",
      "Service Account: 956259099845-compute@developer.gserviceaccount.com\n",
      "Vertex API Parent URI: projects/g360-docai/locations/us-west1\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'g360-docai' # Change to your project id.\n",
    "REGION = 'us-west1' # Change to your region.\n",
    "BUCKET = 'vertex-mlops-chicago-taxi-bucket' # Change to your bucket name.\n",
    "BUCKET_URI = f\"gs://{BUCKET}\" \n",
    "SERVICE_ACCOUNT = \"956259099845-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Bucket URI:\", BUCKET_URI)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "print(\"Vertex API Parent URI:\", PARENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfd33f",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b363e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = f\"v01\"\n",
    "DATASET_DISPLAY_NAME = f\"chicago-taxi-tips\"\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, \"experiments\")\n",
    "RAW_SCHEMA_LOCATION = f\"src/raw_schema/schema.pbtxt\"\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f\"tb-{DATASET_DISPLAY_NAME}\"\n",
    "EXPERIMENT_NAME = f\"{MODEL_DISPLAY_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf63d9",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf8ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/956259099845/locations/us-west1/tensorboards/5002091811125067776/operations/664964841269624832\n",
      "Tensorboard created. Resource name: projects/956259099845/locations/us-west1/tensorboards/5002091811125067776\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/956259099845/locations/us-west1/tensorboards/5002091811125067776')\n",
      "TensorBoard resource name: projects/956259099845/locations/us-west1/tensorboards/5002091811125067776\n"
     ]
    }
   ],
   "source": [
    "tensorboard_resource = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME,\n",
    "                                                       project= PROJECT, \n",
    "                                                        location= REGION)\n",
    "tensorboard_resource_name = tensorboard_resource.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a3859",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90aab80-196d-4f30-a729-eb8f4a23a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket vertex-mlops-chicago-taxi-bucket already exists\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def bucket_check(bucket_name):\n",
    "    \"\"\"Creates a new bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Check if the bucket already exists\n",
    "    if not storage_client.lookup_bucket(bucket_name):\n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "        print(\"Bucket {} created\".format(bucket.name))\n",
    "    else:\n",
    "        print(\"Bucket {} already exists\".format(bucket_name))\n",
    "\n",
    "        \n",
    "bucket_check(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d03fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new experiment artifacts directory...\n",
      "Workspace is ready.\n",
      "Experiment directory: gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments\n"
     ]
    }
   ],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(Path(EXPERIMENT_ARTIFACTS_DIR))\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.makedirs(Path(EXPERIMENT_ARTIFACTS_DIR))\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "print(\"Experiment directory:\", EXPERIMENT_ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08e503",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3aefc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/956259099845/locations/us-west1/metadataStores/default/contexts/chicago-taxi-tips-classifier-v01-run-local-20230519194532 to Experiment: chicago-taxi-tips-classifier-v01\n",
      "Experiment run directory: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    # staging_bucket=BUCKET,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = str(Path(os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)))\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8cee2",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data using Apache Beam\n",
    "\n",
    "The Apache Beam pipeline of data preprocessing is implemented in the [preprocessing](src/preprocessing) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a613a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = str(Path(os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')))\n",
    "TRANSFORMED_DATA_PREFIX = str(Path(os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')))\n",
    "TRANSFORM_ARTIFACTS_DIR = str(Path(os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b81f3",
   "metadata": {},
   "source": [
    "### Get Source Query from Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b8ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 5120\n"
     ]
    }
   ],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    ml_use=ML_USE, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047eac1",
   "metadata": {},
   "source": [
    "### Test Data Preprocessing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db4462e-9c26-4086-a412-c4fe465573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'runner': 'DirectRunner',\n",
    "#     'raw_data_query': raw_data_query,\n",
    "#     'write_raw_data': True,\n",
    "#     'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "#     'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "#     'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "#     'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "#     'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "#     'project': PROJECT\n",
    "# }\n",
    "\n",
    "# args = {\n",
    "#     'runner': 'DirectRunner',\n",
    "#     'raw_data_query': raw_data_query,\n",
    "#     'write_raw_data': True,\n",
    "#     'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "#     'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "#     'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "#     'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "#     'gcs_location': BUCKET_URI,\n",
    "#     'project': PROJECT\n",
    "# }\n",
    "\n",
    "# Start of the code\n",
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "    'project': PROJECT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c225f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6640a98e-38e5-4db1-8323-313f6bfdae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import apache_beam as beam\n",
    "\n",
    "# # Create a PCollection from the instance dicts\n",
    "# pipeline = beam.Pipeline()\n",
    "# instance_dicts_pcoll = pipeline | beam.Create(instance_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ece730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2485: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:324: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:45:37.562629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:37.575066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:37.576706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:37.579156: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:45:37.580767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:37.582416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:37.584073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:38.365555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:38.367380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:38.368913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:45:38.370381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:324: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'raw_data_query': \"\\n    SELECT \\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\n        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\\n        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\\n        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\n        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\\n        tip_bin\\n    FROM playground_us.chicago_taxitrips_prep \\n    WHERE ML_use = 'UNASSIGNED'\\n    LIMIT 5120\", 'write_raw_data': True, 'exported_data_prefix': 'gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/exported_data', 'transformed_data_prefix': 'gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/transformed_data', 'transform_artifact_dir': 'gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/transform_artifacts', 'temporary_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp', 'gcs_location': 'gs://vertex-mlops-chicago-taxi-bucket/bq_tmp'}\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.45.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f234c2e87a0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f234c2e88c0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f234c2e8dd0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f234c2e8e60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f234c2ea050> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f234c2ea0e0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f234c2ea200> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f234c2ea290> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f234c2ea320> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f234c2ea3b0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f234c2ea5f0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f234c2ea710> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f234c2ea560> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f234c2ea680> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f23db3fd790> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/49103f20a56345b49af84288468cf410/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/49103f20a56345b49af84288468cf410/assets\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'g360-docai'>\n",
      " bq show -j --format=prettyjson --project_id=g360-docai None\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'playground_us'\n",
      " projectId: 'g360-docai'\n",
      " tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 5120\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Dataset g360-docai:beam_temp_dataset_7dac23843357490c9dbde31c4c430743 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_2bbe33aa-7_1684525548_712'\n",
      " location: 'US'\n",
      " projectId: 'g360-docai'>\n",
      " bq show -j --format=prettyjson --project_id=g360-docai beam_bq_job_QUERY_BQ_EXPORT_JOB_2bbe33aa-7_1684525548_712\n",
      "INFO:root:Job status: RUNNING\n",
      "INFO:root:Job status: DONE\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2bbe33aa-7_1684525554_497'\n",
      " location: 'US'\n",
      " projectId: 'g360-docai'>\n",
      " bq show -j --format=prettyjson --project_id=g360-docai beam_bq_job_EXPORT_BQ_EXPORT_JOB_2bbe33aa-7_1684525554_497\n",
      "INFO:root:Job status: RUNNING\n",
      "INFO:root:Job status: DONE\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.05609893798828125 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.08843374252319336 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/65528b9df20e4f9dbc6c2c3e56185751/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/65528b9df20e4f9dbc6c2c3e56185751/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10911e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: \"ls\" command does not support \"file://\" URLs. Did you mean to use a gs:// URL?\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d252ab",
   "metadata": {},
   "source": [
    "## 2. Train a custom model locally using a Keras\n",
    "\n",
    "The `Keras` implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c59f7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a212f14",
   "metadata": {},
   "source": [
    "### Read transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "204c8580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropoff_grid_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'euclidean_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'loc_cross_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'payment_type_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'pickup_grid_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'tip_bin': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_day_of_week_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_day_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_hour_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_miles_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'trip_month_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_seconds_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft_output = tft.TFTransformOutput(Path(TRANSFORM_ARTIFACTS_DIR))\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0802ce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_grid_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "euclidean_xf <dtype: 'float32'>: [-0.058890450745821, -0.8328948020935059, -0.8328948020935059]\n",
      "loc_cross_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "payment_type_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "pickup_grid_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "trip_day_of_week_xf <dtype: 'int64'>: [3, 5, 2]\n",
      "trip_day_xf <dtype: 'int64'>: [11, 19, 20]\n",
      "trip_hour_xf <dtype: 'int64'>: [5, 15, 9]\n",
      "trip_miles_xf <dtype: 'float32'>: [-0.019110487774014473, -0.5642716288566589, -0.6047692894935608]\n",
      "trip_month_xf <dtype: 'int64'>: [2, 1, 0]\n",
      "trip_seconds_xf <dtype: 'float32'>: [-0.04297146573662758, -0.3350413143634796, -0.34541943669319153]\n",
      "target: [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_data_file_pattern = os.path.join(Path(TRANSFORMED_DATA_PREFIX),'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(Path(TRANSFORMED_DATA_PREFIX),'eval/data-*.gz')\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f363d4",
   "metadata": {},
   "source": [
    "### Create hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4fced82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_units': [64, 32],\n",
       " 'learning_rate': 0.0001,\n",
       " 'batch_size': 512,\n",
       " 'num_epochs': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb937f2-0b90-497b-9180-2b7e90efc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.common import features\n",
    "\n",
    "# def create_model_inp():\n",
    "#     inputs = {}\n",
    "#     for feature_name in features.FEATURE_NAMES:\n",
    "#         name = features.transformed_name(feature_name)\n",
    "#         if feature_name in features.NUMERICAL_FEATURE_NAMES:\n",
    "#             inputs[name] = keras.layers.Input(name=name, shape=(1,), dtype=tf.float32, ragged=True)\n",
    "#         elif feature_name in features.categorical_feature_names():\n",
    "#             inputs[name] = keras.layers.Input(name=name, shape=(1,), dtype=tf.int64, ragged=True)\n",
    "#         else:\n",
    "#             pass\n",
    "#     return inputs\n",
    "\n",
    "# def _cbc(feature_vocab_sizes, hyperparams):\n",
    "#     input_layers = create_model_inp()\n",
    "#     layers = []\n",
    "#     for key in input_layers:\n",
    "#         feature_name = features.original_name(key)\n",
    "#         if feature_name in features.EMBEDDING_CATEGORICAL_FEATURES:\n",
    "#             vocab_size = feature_vocab_sizes[feature_name]\n",
    "#             embedding_size = features.EMBEDDING_CATEGORICAL_FEATURES[feature_name]\n",
    "#             embedding_output = keras.layers.Embedding(\n",
    "#                 input_dim=vocab_size + 1,\n",
    "#                 output_dim=embedding_size,\n",
    "#                 name=f\"{key}_embedding\",\n",
    "#             )(input_layers[key])\n",
    "#             layers.append(embedding_output)\n",
    "#         elif feature_name in features.ONEHOT_CATEGORICAL_FEATURE_NAMES:\n",
    "#             vocab_size = feature_vocab_sizes[feature_name]\n",
    "#             onehot_layer = keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "#                 num_tokens=vocab_size + 1,\n",
    "#                 output_mode=\"binary\",\n",
    "#                 name=f\"{key}_onehot\",\n",
    "#             )(input_layers[key])\n",
    "#             layers.append(onehot_layer)\n",
    "#         elif feature_name in features.NUMERICAL_FEATURE_NAMES:\n",
    "#             numeric_layer = keras.layers.Reshape((-1,), name=f\"{key}_numeric\")(input_layers[key])  # Use Reshape with (-1,) shape\n",
    "#             layers.append(numeric_layer)\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#     reshaped_layers = []\n",
    "#     for layer in layers:\n",
    "#         if len(layer.shape) > 2:\n",
    "#             reshaped_layer = keras.layers.Reshape((-1,))(layer)\n",
    "#         else:\n",
    "#             reshaped_layer = layer\n",
    "#         reshaped_layers.append(reshaped_layer)\n",
    "\n",
    "#     joined = keras.layers.Concatenate(name=\"combines_inputs\", axis=-1)(reshaped_layers)\n",
    "#     feedforward_output = keras.Sequential(\n",
    "#         [\n",
    "#             keras.layers.Dense(units, activation=\"relu\")\n",
    "#             for units in hyperparams[\"hidden_units\"]\n",
    "#         ],\n",
    "#         name=\"feedforward_network\",\n",
    "#     )(joined)\n",
    "#     logits = keras.layers.Dense(units=1, name=\"logits\")(feedforward_output)\n",
    "\n",
    "#     model = keras.Model(inputs=input_layers, outputs=[logits])\n",
    "#     return model\n",
    "\n",
    "# def create_binary(tft_output, hyperparams):\n",
    "#     feature_vocab_sizes = dict()\n",
    "#     for feature_name in features.categorical_feature_names():\n",
    "#         feature_vocab_sizes[feature_name] = tft_output.vocabulary_size_by_name(\n",
    "#             feature_name\n",
    "#         )\n",
    "#     return _cbc(feature_vocab_sizes, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a8d9829-c9e1-4306-a56d-fee795de8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classy = create_binary(tft_output, hyperparams)\n",
    "# classy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17b65e",
   "metadata": {},
   "source": [
    "### Create and test model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a05b0f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " trip_month_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_day_xf (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_hour_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_grid_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " loc_cross_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_month_xf_embedding (Embed  (None, 1, 2)        18          ['trip_month_xf[0][0]']          \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " trip_day_xf_embedding (Embeddi  (None, 1, 4)        128         ['trip_day_xf[0][0]']            \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf (InputLaye  [(None, 1)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " trip_hour_xf_embedding (Embedd  (None, 1, 3)        75          ['trip_hour_xf[0][0]']           \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " trip_seconds_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_miles_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " payment_type_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf_embedding (Embe  (None, 1, 3)        9           ['pickup_grid_xf[0][0]']         \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " dropoff_grid_xf_embedding (Emb  (None, 1, 3)        12          ['dropoff_grid_xf[0][0]']        \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " euclidean_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " loc_cross_xf_embedding (Embedd  (None, 1, 10)       60          ['loc_cross_xf[0][0]']           \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 2)            0           ['trip_month_xf_embedding[0][0]']\n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 4)            0           ['trip_day_xf_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf_onehot (Ca  (None, 8)           0           ['trip_day_of_week_xf[0][0]']    \n",
      " tegoryEncoding)                                                                                  \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 3)            0           ['trip_hour_xf_embedding[0][0]'] \n",
      "                                                                                                  \n",
      " trip_seconds_xf_numeric (Resha  (None, 1)           0           ['trip_seconds_xf[0][0]']        \n",
      " pe)                                                                                              \n",
      "                                                                                                  \n",
      " trip_miles_xf_numeric (Reshape  (None, 1)           0           ['trip_miles_xf[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " payment_type_xf_onehot (Catego  (None, 7)           0           ['payment_type_xf[0][0]']        \n",
      " ryEncoding)                                                                                      \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 3)            0           ['pickup_grid_xf_embedding[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 3)            0           ['dropoff_grid_xf_embedding[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " euclidean_xf_numeric (Reshape)  (None, 1)           0           ['euclidean_xf[0][0]']           \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 10)           0           ['loc_cross_xf_embedding[0][0]'] \n",
      "                                                                                                  \n",
      " combines_inputs (Concatenate)  (None, 43)           0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]',              \n",
      "                                                                  'trip_day_of_week_xf_onehot[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'reshape_2[0][0]',              \n",
      "                                                                  'trip_seconds_xf_numeric[0][0]',\n",
      "                                                                  'trip_miles_xf_numeric[0][0]',  \n",
      "                                                                  'payment_type_xf_onehot[0][0]', \n",
      "                                                                  'reshape_3[0][0]',              \n",
      "                                                                  'reshape_4[0][0]',              \n",
      "                                                                  'euclidean_xf_numeric[0][0]',   \n",
      "                                                                  'reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " feedforward_network (Sequentia  (None, 32)          4896        ['combines_inputs[0][0]']        \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " logits (Dense)                 (None, 1)            33          ['feedforward_network[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,231\n",
      "Trainable params: 5,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = model.create_binary_classifier(tft_output, hyperparams)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad4d33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06e056e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.10309295],\n",
       "       [0.10229045],\n",
       "       [0.08642562]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d451ef7",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "153efc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 5\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "# hyperparameters_str = ','.join(f\"{key}:{value}\" for key, value in hyperparams.items())\n",
    "# vertex_ai.log_params(hyperparameters_str)\n",
    "\n",
    "# vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a91c4924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tft output from gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/transform_artifacts\n",
      "INFO:root:Model training started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:46:17.634398: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5588b778ea90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-19 19:46:17.634456: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-05-19 19:46:17.640633: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-19 19:46:17.798600: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 210ms/step - loss: 0.6578 - accuracy: 0.8918 - val_loss: 0.6011 - val_accuracy: 0.9004\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.5649 - accuracy: 0.8926 - val_loss: 0.5201 - val_accuracy: 0.8965\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4883 - accuracy: 0.8926 - val_loss: 0.4283 - val_accuracy: 0.9238\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.4113 - accuracy: 0.8923 - val_loss: 0.3466 - val_accuracy: 0.9238\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3393 - accuracy: 0.8923 - val_loss: 0.2875 - val_accuracy: 0.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model training completed.\n"
     ]
    }
   ],
   "source": [
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebbce8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading raw schema from src/raw_schema/schema.pbtxt\n",
      "INFO:root:Loading tft output from gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/transform_artifacts\n",
      "INFO:root:Model evaluation started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2842 - accuracy: 0.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(\n",
    "    model=classifier,\n",
    "    data_dir=eval_data_file_pattern,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_metrics(\n",
    "    {\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cff3c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:46:24.453187: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:46:25.416259: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:46:25.416353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:46:25.416371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-19 19:46:26.322111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:46:26.333555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:46:26.335353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "View your Tensorboard at https://us-west1.tensorboard.googleusercontent.com/experiment/projects+956259099845+locations+us-west1+tensorboards+5002091811125067776+experiments+chicago-taxi-tips-classifier-v01\n",
      "\u001b[1m[2023-05-19T19:46:26]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2023-05-19T19:46:27]\u001b[0m Total uploaded: 30 scalars, 0 tensors, 1 binary objects (149.3 kB)\n",
      "\u001b[2K\u001b[33mListening for new data in logdir...\u001b[0m\r"
     ]
    }
   ],
   "source": [
    "!tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44268d04",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2cf46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "INFO:root:Model export started...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519194532/model/assets\n",
      "INFO:root:Model export completed.\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1ba3e",
   "metadata": {},
   "source": [
    "### Inspect model serving signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb8989c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:46:34.832577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:46:35.896790: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:46:35.896914: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:46:35.896933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['examples'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_tf_example_examples:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['probabilities'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall_25:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7517a78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:46:38.228018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:46:39.299049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:46:39.299137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:46:39.299170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['dropoff_grid'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_dropoff_grid:0\n",
      "  inputs['euclidean'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_euclidean:0\n",
      "  inputs['loc_cross'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_loc_cross:0\n",
      "  inputs['payment_type'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_payment_type:0\n",
      "  inputs['pickup_grid'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_pickup_grid:0\n",
      "  inputs['trip_day'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_day:0\n",
      "  inputs['trip_day_of_week'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_day_of_week:0\n",
      "  inputs['trip_hour'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_hour:0\n",
      "  inputs['trip_miles'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_miles:0\n",
      "  inputs['trip_month'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_month:0\n",
      "  inputs['trip_seconds'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_seconds:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['classes'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall_24:0\n",
      "  outputs['scores'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall_24:1\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68ecd0",
   "metadata": {},
   "source": [
    "### Test the exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "965d7d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model is loaded.\n"
     ]
    }
   ],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)\n",
    "print(\"Saved model is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "395f6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: [[0.16363572]\n",
      " [0.22309399]\n",
      " [0.19801933]]\n"
     ]
    }
   ],
   "source": [
    "# Test the serving_tf_example with TF Examples\n",
    "\n",
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '/data-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41f041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serving_default with feature dictionary\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d51e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": 12,\n",
    "    \"trip_hour\": 6,\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day_of_week\": 4,\n",
    "    \"trip_seconds\": 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6469de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: [[b'tip<20%' b'tip>=20%']]\n",
      "scores: [[0.38365754 0.6163424 ]]\n"
     ]
    }
   ],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec2080",
   "metadata": {},
   "source": [
    "## Start a new Vertex AI experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d87d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/956259099845/locations/us-west1/metadataStores/default/contexts/chicago-taxi-tips-classifier-v01-run-gcp-20230519194644 to Experiment: chicago-taxi-tips-classifier-v01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/956259099845/locations/us-west1/metadataStores/default/contexts/chicago-taxi-tips-classifier-v01-run-gcp-20230519194644 to Experiment: chicago-taxi-tips-classifier-v01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment run directory: gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME)\n",
    "\n",
    "run_id = f\"run-gcp-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade27b7",
   "metadata": {},
   "source": [
    "## 3. Submit a Data Processing Job to Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d903ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fac3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 5000\n"
     ]
    }
   ],
   "source": [
    "# ML_USE = 'UNASSIGNED'\n",
    "# # LIMIT = 1000000\n",
    "# LIMIT = 5000\n",
    "# raw_data_query = datasource_utils.get_training_source_query(\n",
    "#     project=PROJECT, \n",
    "#     region=REGION, \n",
    "#     dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "#     ml_use=ML_USE, \n",
    "#     limit=LIMIT\n",
    "# )\n",
    "# print(raw_data_query)\n",
    "\n",
    "# etl_job_name = f\"etl-{MODEL_DISPLAY_NAME}-{run_id}\"\n",
    "\n",
    "# args = {\n",
    "#     'job_name': etl_job_name,\n",
    "#     'runner': 'DataflowRunner',\n",
    "#     'raw_data_query': raw_data_query,\n",
    "#     'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "#     'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "#     'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "#     'write_raw_data': False,\n",
    "#     'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "#     'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "#     'project': PROJECT,\n",
    "#     'region': REGION,\n",
    "#     'setup_file': './setup.py'\n",
    "# }\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "ML_USE = 'UNASSIGNED'\n",
    "# LIMIT = 1000000\n",
    "LIMIT = 5000\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    ml_use=ML_USE, \n",
    "    limit=LIMIT\n",
    ")\n",
    "print(raw_data_query)\n",
    "\n",
    "etl_job_name = f\"etl-{MODEL_DISPLAY_NAME}-{run_id}\"\n",
    "\n",
    "args = {\n",
    "    'job_name': etl_job_name,\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c909d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "014e5512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing started...\n",
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2485: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/41891012e1174466aa6e9e594befe201/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/41891012e1174466aa6e9e594befe201/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/e08ca86faabc40f28f71eab14ed2d54d/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/e08ca86faabc40f28f71eab14ed2d54d/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92a21c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4dfa8",
   "metadata": {},
   "source": [
    "## 4. Submit a Custom Training Job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50db6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e998139",
   "metadata": {},
   "source": [
    "### Test the training task locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9382ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 19:47:21.029684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:47:22.108378: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:47:22.108485: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 19:47:22.108505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO:root:Python Version = 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n",
      "[GCC 9.4.0]\n",
      "INFO:root:TensorFlow Version = 2.11.0\n",
      "INFO:root:TF_CONFIG = Not found\n",
      "2023-05-19 19:47:25.670660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 19:47:25.704003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:47:25.716812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:47:25.718610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:47:26.485669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:47:26.487333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:47:26.488766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 19:47:26.490164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 340 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "INFO:root:DEVICES = [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14351671614424622169\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 357040128\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16674102291995666122\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "INFO:root:Task started...\n",
      "INFO:root:Hyperparameter: {'model_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/model', 'log_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/logs', 'train_data_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/transformed_data/train/*', 'eval_data_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/transformed_data/eval/*', 'tft_output_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/transform_artifacts', 'learning_rate': 0.001, 'batch_size': 512, 'hidden_units': [32, 32], 'num_epochs': 3, 'project': 'g360-docai', 'region': 'us-west1', 'staging_bucket': 'vertex-mlops-chicago-taxi-bucket', 'experiment_name': 'chicago-taxi-tips-classifier-v01', 'run_name': 'run-gcp-20230519194644'}\n",
      "INFO:root:Using Vertex AI experiment: chicago-taxi-tips-classifier-v01\n",
      "Associating projects/956259099845/locations/us-central1/metadataStores/default/contexts/chicago-taxi-tips-classifier-v01-run-gcp-20230519194644 to Experiment: chicago-taxi-tips-classifier-v01\n",
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/956259099845/locations/us-central1/metadataStores/default/contexts/chicago-taxi-tips-classifier-v01-run-gcp-20230519194644 to Experiment: chicago-taxi-tips-classifier-v01\n",
      "INFO:root:Run run-gcp-20230519194644 started.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/mlops-with-vertex-ai/src/model_training/task.py\", line 155, in <module>\n",
      "    main()\n",
      "  File \"/home/jupyter/mlops-with-vertex-ai/src/model_training/task.py\", line 103, in main\n",
      "    vertex_ai.log_params(hyperparams)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/google/cloud/aiplatform/metadata/metadata.py\", line 548, in log_params\n",
      "    self.experiment_run.log_params(params=params)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/google/cloud/aiplatform/metadata/experiment_run_resource.py\", line 1013, in log_params\n",
      "    f\"Value for key {key} is of type {type(value).__name__} but must be one of float, int, str\"\n",
      "TypeError: Value for key hidden_units is of type list but must be one of float, int, str\n"
     ]
    }
   ],
   "source": [
    "!python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTIFACTS_DIR} \\\n",
    "    --num-epochs=3 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077e9d4",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d526a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer package upload location: gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/trainer_packages\n"
     ]
    }
   ],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')\n",
    "TRAINER_PACKAGE_NAME = f'{MODEL_DISPLAY_NAME}_trainer'\n",
    "print(\"Trainer package upload location:\", TRAINER_PACKAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "550cc9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/.ipynb_checkpoints/': No such file or directory\n",
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n",
      "chicago-taxi-tips-classifier-v01_trainer/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/training_pipeline.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/components.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/prediction_pipeline.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/runner.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tfx_pipelines/config.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/datasource_utils_tests.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/model_deployment_tests.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/etl_tests.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/pipeline_deployment_tests.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/tests/model_tests.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/__pycache__/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/__pycache__/transformations.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/__pycache__/etl.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/__pycache__/__init__.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/transformations.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/etl.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/.ipynb_checkpoints/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/.ipynb_checkpoints/etl-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/preprocessing/.ipynb_checkpoints/transformations-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/model.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/data.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/data.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/model.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/defaults.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/task.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/exporter.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/__init__.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/model.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/defaults.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/trainer.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/__init__.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/trainer.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__pycache__/exporter.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/defaults.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/task.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/trainer.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/exporter.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/runner.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/data-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/defaults-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/exporter-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/trainer-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/runner-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/task-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/.ipynb_checkpoints/model-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/model_training/data.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/pipeline_triggering/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/pipeline_triggering/main.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/pipeline_triggering/requirements.txt\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/pipeline_triggering/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/pipeline_triggering/.ipynb_checkpoints/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/pipeline_triggering/.ipynb_checkpoints/main-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/features.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/__init__.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/datasource_utils.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/__init__.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/features.cpython-39.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__pycache__/datasource_utils.cpython-37.pyc\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/features.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/datasource_utils.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/__init__.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/.ipynb_checkpoints/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/.ipynb_checkpoints/datasource_utils-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/common/.ipynb_checkpoints/features-checkpoint.py\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/raw_schema/\n",
      "chicago-taxi-tips-classifier-v01_trainer/src/raw_schema/schema.pbtxt\n",
      "chicago-taxi-tips-classifier-v01_trainer/setup.py\n",
      "Copying file://chicago-taxi-tips-classifier-v01_trainer.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][ 34.4 KiB/ 34.4 KiB]                                                \n",
      "Operation completed over 1 objects/34.4 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!rm -r src/__pycache__/\n",
    "!rm -r src/.ipynb_checkpoints/\n",
    "!rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}.tar.gz\n",
    "\n",
    "!mkdir {TRAINER_PACKAGE_NAME}\n",
    "\n",
    "!cp setup.py {TRAINER_PACKAGE_NAME}/\n",
    "!cp -r src {TRAINER_PACKAGE_NAME}/\n",
    "!tar cvf {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}\n",
    "!gzip {TRAINER_PACKAGE_NAME}.tar\n",
    "!gsutil cp {TRAINER_PACKAGE_NAME}.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r {TRAINER_PACKAGE_NAME}\n",
    "!rm -r {TRAINER_PACKAGE_NAME}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3af757",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e01bf43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image: us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-5:latest\n"
     ]
    }
   ],
   "source": [
    "TRAIN_RUNTIME = 'tf-cpu.2-5'\n",
    "TRAIN_IMAGE = f\"us-docker.pkg.dev/vertex-ai/training/{TRAIN_RUNTIME}:latest\"\n",
    "print(\"Training image:\", TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d18dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_units = \"64,64\"\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX + \"/train/*\"}',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX + \"/eval/*\"}',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTIFACTS_DIR}',\n",
    "    f'--num-epochs={num_epochs}',\n",
    "    f'--learning-rate={learning_rate}',\n",
    "    f'--project={PROJECT}',\n",
    "    f'--region={REGION}',\n",
    "    f'--staging-bucket={BUCKET}',\n",
    "    f'--experiment-name={EXPERIMENT_NAME}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a30d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_uri = os.path.join(TRAINER_PACKAGE_DIR, f'{TRAINER_PACKAGE_NAME}.tar.gz')\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": 'n1-standard-4',\n",
    "            \"accelerator_count\": 0\n",
    "    },\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": \"src.model_training.task\",\n",
    "            \"args\": trainer_args,\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d6e99",
   "metadata": {},
   "source": [
    "### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4544105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting a custom training job...\n",
      "Creating CustomJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob created. Resource name: projects/956259099845/locations/us-west1/customJobs/2308382881073856512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/956259099845/locations/us-west1/customJobs/2308382881073856512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this CustomJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_job = aiplatform.CustomJob.get('projects/956259099845/locations/us-west1/customJobs/2308382881073856512')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/956259099845/locations/us-west1/customJobs/2308382881073856512')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/2308382881073856512?project=956259099845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/2308382881073856512?project=956259099845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Tensorboard:\n",
      "https://us-west1.tensorboard.googleusercontent.com/experiment/projects+956259099845+locations+us-west1+tensorboards+5002091811125067776+experiments+2308382881073856512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-west1.tensorboard.googleusercontent.com/experiment/projects+956259099845+locations+us-west1+tensorboards+5002091811125067776+experiments+2308382881073856512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/956259099845/locations/us-west1/customJobs/2308382881073856512 current state:\n",
      "JobState.JOB_STATE_FAILED\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=956259099845&resource=ml_job%2Fjob_id%2F2308382881073856512&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%222308382881073856512%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_814/715272727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSERVICE_ACCOUNT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorboard_resource_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, experiment, experiment_run, tensorboard, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   1691\u001b[0m             \u001b[0mtensorboard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, experiment, experiment_run, tensorboard, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         )\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m     def submit(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_JOB_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=956259099845&resource=ml_job%2Fjob_id%2F2308382881073856512&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%222308382881073856512%22\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Submitting a custom training job...\")\n",
    "\n",
    "training_job_display_name = f\"{TRAINER_PACKAGE_NAME}_{run_id}\"\n",
    "\n",
    "training_job = vertex_ai.CustomJob(\n",
    "    display_name=training_job_display_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=EXPERIMENT_RUN_DIR,\n",
    ")\n",
    "\n",
    "training_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2896b59",
   "metadata": {},
   "source": [
    "## 5. Upload exported model to Vertex AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e86b288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e0c39",
   "metadata": {},
   "source": [
    "### Generate the Explanation metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3d5df4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': {'trip_month': {'input_tensor_name': 'trip_month',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'trip_day': {'input_tensor_name': 'trip_day',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'trip_day_of_week': {'input_tensor_name': 'trip_day_of_week',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'trip_hour': {'input_tensor_name': 'trip_hour',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'trip_seconds': {'input_tensor_name': 'trip_seconds', 'modality': 'numeric'},\n",
       "  'trip_miles': {'input_tensor_name': 'trip_miles', 'modality': 'numeric'},\n",
       "  'payment_type': {'input_tensor_name': 'payment_type',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'pickup_grid': {'input_tensor_name': 'pickup_grid',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'dropoff_grid': {'input_tensor_name': 'dropoff_grid',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'},\n",
       "  'euclidean': {'input_tensor_name': 'euclidean', 'modality': 'numeric'},\n",
       "  'loc_cross': {'input_tensor_name': 'loc_cross',\n",
       "   'encoding': 'IDENTITY',\n",
       "   'modality': 'categorical'}},\n",
       " 'outputs': {'scores': {'output_tensor_name': 'scores'}},\n",
       " 'params': {'sampled_shapley_attribution': {'path_count': 10}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_config = features.generate_explanation_config()\n",
    "explanation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd0587",
   "metadata": {},
   "source": [
    "### Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6d0dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving image: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n"
     ]
    }
   ],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-5'\n",
    "SERVING_IMAGE = f\"us-docker.pkg.dev/vertex-ai/prediction/{SERVING_RUNTIME}:latest\"\n",
    "print(\"Serving image:\", SERVING_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b24abd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 There are no files in directory \"gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/model\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    922\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"There are no files in directory \"gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/model\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory.\"\n\tdebug_error_string = \"{\"created\":\"@1684526008.521502938\",\"description\":\"Error received from peer ipv4:74.125.197.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"There are no files in directory \"gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/model\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory.\",\"grpc_status\":5}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_814/3814723402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     labels={\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m'dataset_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDATASET_DISPLAY_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;34m'experiment'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(cls, serving_container_image_uri, artifact_uri, model_id, parent_model, is_default_version, version_aliases, version_description, serving_container_predict_route, serving_container_health_route, description, serving_container_command, serving_container_args, serving_container_environment_variables, serving_container_ports, local_model, instance_schema_uri, parameters_schema_uri, prediction_schema_uri, explanation_metadata, explanation_parameters, display_name, project, location, credentials, labels, encryption_spec_key_name, staging_bucket, sync, upload_request_timeout)\u001b[0m\n\u001b[1;32m   3155\u001b[0m         lro = api_client.upload_model(\n\u001b[1;32m   3156\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupload_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m         )\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py\u001b[0m in \u001b[0;36mupload_model\u001b[0;34m(self, request, parent, model, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 There are no files in directory \"gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-gcp-20230519194644/model\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory."
     ]
    }
   ],
   "source": [
    "explanation_metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "    inputs=explanation_config[\"inputs\"],\n",
    "    outputs=explanation_config[\"outputs\"],\n",
    ")\n",
    "explanation_parameters = vertex_ai.explain.ExplanationParameters(\n",
    "    explanation_config[\"params\"]\n",
    ")\n",
    "\n",
    "vertex_model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=SERVING_IMAGE,\n",
    "    parameters_schema_uri=None,\n",
    "    instance_schema_uri=None,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters,\n",
    "    labels={\n",
    "        'dataset_name': DATASET_DISPLAY_NAME,\n",
    "        'experiment': run_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51756dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model.gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa55220",
   "metadata": {},
   "source": [
    "## 6. Extract experiment run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07808f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = vertex_ai.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/vertex-ai/locations{REGION}/experiments/{EXPERIMENT_NAME}/metrics?project={PROJECT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96618f6",
   "metadata": {},
   "source": [
    "## 7. Submit a Hyperparameter Tuning Job to Vertex AI\n",
    "\n",
    "For more information about configuring a hyperparameter study, refer to [Vertex AI Hyperparameter job configuration](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b87cd",
   "metadata": {},
   "source": [
    "### Configure a hyperparameter job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_spec = {\n",
    "    'ACCURACY': 'maximize'\n",
    "}\n",
    "\n",
    "parameter_spec = {\n",
    "    'learning-rate': hp_tuning.DoubleParameterSpec(min=0.0001, max=0.01, scale='log'),\n",
    "    'hidden-units': hp_tuning.CategoricalParameterSpec(values=[\"32,32\", \"64,64\", \"128,128\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2454dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_display_name = f\"hpt_{TRAINER_PACKAGE_NAME}_{run_id}\"\n",
    "\n",
    "hp_tuning_job = vertex_ai.HyperparameterTuningJob(\n",
    "    display_name=tuning_job_display_name,\n",
    "    custom_job=training_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=4,\n",
    "    parallel_trial_count=2,\n",
    "    search_algorithm=None # Bayesian optimization.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4ee22",
   "metadata": {},
   "source": [
    "### Submit the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a hyperparameter tunning job...\")\n",
    "\n",
    "hp_tuning_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd20da",
   "metadata": {},
   "source": [
    "### Retrieve trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8352e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tuning_job.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311922be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = sorted(\n",
    "    hp_tuning_job.trials, \n",
    "    key=lambda trial: trial.final_measurement.metrics[0].value, \n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "print(\"Best trial ID:\", best_trial.id)\n",
    "print(\"Validation Accuracy:\", best_trial.final_measurement.metrics[0].value)\n",
    "print(\"Hyperparameter Values:\")\n",
    "for parameter in best_trial.parameters:\n",
    "    print(f\" - {parameter.parameter_id}:{parameter.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88555c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
