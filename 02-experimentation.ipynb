{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae86f7",
   "metadata": {},
   "source": [
    "# 02 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/ai-platform-unified/docs/training/custom-training) to train a keras classifier to predict whether a given trip will result in a tip > 20%. The notebook covers the following tasks:\n",
    "1. Preprocess the data locally using Apache Beam.\n",
    "2. Train and test custom model locally using a Keras implementation.\n",
    "3. Submit a Dataflow job to preprocess the data at scale.\n",
    "4. Submit a custom training job to Vertex AI using a [pre-built container](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "5. Upload the trained model to Vertex AI.\n",
    "6. Track experiment parameters from [Vertex AI Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction).\n",
    "7. Submit a [hyperparameter tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to Vertex AI.\n",
    "\n",
    "We use [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) \n",
    "and [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) to  track, visualize, and compare ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5205917",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c6cfe",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4171d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 00:58:56.641175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 00:58:57.610193: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 00:58:57.610316: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-19 00:58:57.610328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.11.0\n",
      "TensorFlow Transform: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hp_tuning\n",
    "\n",
    "from src.common import features, datasource_utils\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba0da4-aaa0-4984-b605-a8fe0e9927cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e8329-2477-41df-801e-414d63f12f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ebd2f-dbfa-446e-8615-737f6cc42668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bc9635-2194-4d7f-b1e3-1e9ef2c2228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7f0315-133f-4d9b-a4f7-73a77a48b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.11.0\n",
      "TensorFlow Transform: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde361cf",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fce242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: g360-docai\n",
      "Region: us-west1\n",
      "Bucket name: vertex-mlops-chicago-taxi-bucket\n",
      "Bucket URI: gs://vertex-mlops-chicago-taxi-bucket\n",
      "Service Account: 956259099845-compute@developer.gserviceaccount.com\n",
      "Vertex API Parent URI: projects/g360-docai/locations/us-west1\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'g360-docai' # Change to your project id.\n",
    "REGION = 'us-west1' # Change to your region.\n",
    "BUCKET = 'vertex-mlops-chicago-taxi-bucket' # Change to your bucket name.\n",
    "BUCKET_URI = f\"gs://{BUCKET}\" \n",
    "SERVICE_ACCOUNT = \"956259099845-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Bucket URI:\", BUCKET_URI)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "print(\"Vertex API Parent URI:\", PARENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfd33f",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b363e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = f\"v01\"\n",
    "DATASET_DISPLAY_NAME = f\"chicago-taxi-tips\"\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "\n",
    "WORKSPACE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, \"experiments\")\n",
    "RAW_SCHEMA_LOCATION = f\"src/raw_schema/schema.pbtxt\"\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f\"tb-{DATASET_DISPLAY_NAME}\"\n",
    "EXPERIMENT_NAME = f\"{MODEL_DISPLAY_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf63d9",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf8ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/956259099845/locations/us-central1/tensorboards/1722723614492459008/operations/6372026330887749632\n",
      "Tensorboard created. Resource name: projects/956259099845/locations/us-central1/tensorboards/1722723614492459008\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/956259099845/locations/us-central1/tensorboards/1722723614492459008')\n",
      "TensorBoard resource name: projects/956259099845/locations/us-central1/tensorboards/1722723614492459008\n"
     ]
    }
   ],
   "source": [
    "tensorboard_resource = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "tensorboard_resource_name = tensorboard_resource.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a3859",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90aab80-196d-4f30-a729-eb8f4a23a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket vertex-mlops-chicago-taxi-bucket already exists\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def bucket_check(bucket_name):\n",
    "    \"\"\"Creates a new bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Check if the bucket already exists\n",
    "    if not storage_client.lookup_bucket(bucket_name):\n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "        print(\"Bucket {} created\".format(bucket.name))\n",
    "    else:\n",
    "        print(\"Bucket {} already exists\".format(bucket_name))\n",
    "\n",
    "        \n",
    "bucket_check(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d03fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new experiment artifacts directory...\n",
      "Workspace is ready.\n",
      "Experiment directory: gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments\n"
     ]
    }
   ],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(Path(EXPERIMENT_ARTIFACTS_DIR))\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.makedirs(Path(EXPERIMENT_ARTIFACTS_DIR))\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "print(\"Experiment directory:\", EXPERIMENT_ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08e503",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3aefc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/956259099845/locations/us-west1/metadataStores/default/contexts/chicago-taxi-tips-classifier-v01-run-local-20230519005902 to Experiment: chicago-taxi-tips-classifier-v01\n",
      "Experiment run directory: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519005902\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    # staging_bucket=BUCKET,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = str(Path(os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)))\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8cee2",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data using Apache Beam\n",
    "\n",
    "The Apache Beam pipeline of data preprocessing is implemented in the [preprocessing](src/preprocessing) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a613a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = str(Path(os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')))\n",
    "TRANSFORMED_DATA_PREFIX = str(Path(os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')))\n",
    "TRANSFORM_ARTIFACTS_DIR = str(Path(os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b81f3",
   "metadata": {},
   "source": [
    "### Get Source Query from Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b8ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 5120\n"
     ]
    }
   ],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    ml_use=ML_USE, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047eac1",
   "metadata": {},
   "source": [
    "### Test Data Preprocessing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db4462e-9c26-4086-a412-c4fe465573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {\n",
    "#     'runner': 'DirectRunner',\n",
    "#     'raw_data_query': raw_data_query,\n",
    "#     'write_raw_data': True,\n",
    "#     'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "#     'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "#     'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "#     'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "#     'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "#     'project': PROJECT\n",
    "# }\n",
    "\n",
    "# args = {\n",
    "#     'runner': 'DirectRunner',\n",
    "#     'raw_data_query': raw_data_query,\n",
    "#     'write_raw_data': True,\n",
    "#     'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "#     'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "#     'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "#     'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "#     'gcs_location': BUCKET_URI,\n",
    "#     'project': PROJECT\n",
    "# }\n",
    "\n",
    "# Start of the code\n",
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "    'project': PROJECT\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c225f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6640a98e-38e5-4db1-8323-313f6bfdae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import apache_beam as beam\n",
    "\n",
    "# # Create a PCollection from the instance dicts\n",
    "# pipeline = beam.Pipeline()\n",
    "# instance_dicts_pcoll = pipeline | beam.Create(instance_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ece730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2485: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n",
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:324: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 00:59:06.512932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:06.525679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:06.527377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:06.529782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 00:59:06.531083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:06.532701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:06.534263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:07.304461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:07.306256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:07.307783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 00:59:07.309185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:324: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'raw_data_query': \"\\n    SELECT \\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\n        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\\n        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\\n        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\n        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\\n        tip_bin\\n    FROM playground_us.chicago_taxitrips_prep \\n    WHERE ML_use = 'UNASSIGNED'\\n    LIMIT 5120\", 'write_raw_data': True, 'exported_data_prefix': 'gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519005902/exported_data', 'transformed_data_prefix': 'gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519005902/transformed_data', 'transform_artifact_dir': 'gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/experiments/chicago-taxi-tips-classifier-v01/run-local-20230519005902/transform_artifacts', 'temporary_dir': 'gs://vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp', 'gcs_location': 'gs://vertex-mlops-chicago-taxi-bucket/bq_tmp'}\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.45.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7fc9d07eb710> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7fc9d07eb830> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fc9d07ebd40> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7fc9d07ebdd0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7fc9d07ebf80> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7fc9d07ec050> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7fc9d07ec170> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7fc9d07ec200> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7fc9d07ec290> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7fc9d07ec320> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fc9d07ec560> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7fc9d07ec680> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7fc9d07ec4d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7fc9d07ec5f0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fca5bd9efd0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'g360-docai'>\n",
      " bq show -j --format=prettyjson --project_id=g360-docai None\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'playground_us'\n",
      " projectId: 'g360-docai'\n",
      " tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM playground_us.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 5120\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Dataset g360-docai:beam_temp_dataset_ad6a0ff8654e4f48a8f6956f8f62742f does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_307ac48a-b_1684457956_452'\n",
      " location: 'US'\n",
      " projectId: 'g360-docai'>\n",
      " bq show -j --format=prettyjson --project_id=g360-docai beam_bq_job_QUERY_BQ_EXPORT_JOB_307ac48a-b_1684457956_452\n",
      "INFO:root:Job status: RUNNING\n",
      "INFO:root:Job status: DONE\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_307ac48a-b_1684457961_89'\n",
      " location: 'US'\n",
      " projectId: 'g360-docai'>\n",
      " bq show -j --format=prettyjson --project_id=g360-docai beam_bq_job_EXPORT_BQ_EXPORT_JOB_307ac48a-b_1684457961_89\n",
      "INFO:root:Job status: RUNNING\n",
      "INFO:root:Job status: DONE\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.05428767204284668 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/91334bb9040b4a21be5b443edcd0faec/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/91334bb9040b4a21be5b443edcd0faec/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.045641422271728516 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/864f0945a07a430792d171a6ff8895b6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs:/vertex-mlops-chicago-taxi-bucket/chicago-taxi-tips/tmp/tftransform_tmp/864f0945a07a430792d171a6ff8895b6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10911e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: \"ls\" command does not support \"file://\" URLs. Did you mean to use a gs:// URL?\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d252ab",
   "metadata": {},
   "source": [
    "## 2. Train a custom model locally using a Keras\n",
    "\n",
    "The `Keras` implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c59f7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a212f14",
   "metadata": {},
   "source": [
    "### Read transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "204c8580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropoff_grid_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'euclidean_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'loc_cross_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'payment_type_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'pickup_grid_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'tip_bin': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_day_of_week_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_day_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_hour_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_miles_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " 'trip_month_xf': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " 'trip_seconds_xf': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft_output = tft.TFTransformOutput(Path(TRANSFORM_ARTIFACTS_DIR))\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0802ce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_grid_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "euclidean_xf <dtype: 'float32'>: [-0.8328947424888611, -0.8328947424888611, -0.8328947424888611]\n",
      "loc_cross_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "payment_type_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "pickup_grid_xf <dtype: 'int64'>: [0, 0, 0]\n",
      "trip_day_of_week_xf <dtype: 'int64'>: [0, 1, 4]\n",
      "trip_day_xf <dtype: 'int64'>: [14, 7, 12]\n",
      "trip_hour_xf <dtype: 'int64'>: [0, 14, 23]\n",
      "trip_miles_xf <dtype: 'float32'>: [-0.5798476338386536, -0.40228089690208435, -0.4614697992801666]\n",
      "trip_month_xf <dtype: 'int64'>: [3, 2, 3]\n",
      "trip_seconds_xf <dtype: 'float32'>: [-0.341960072517395, -0.2984708249568939, -0.2836449444293976]\n",
      "target: [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_data_file_pattern = os.path.join(Path(TRANSFORMED_DATA_PREFIX),'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(Path(TRANSFORMED_DATA_PREFIX),'eval/data-*.gz')\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f363d4",
   "metadata": {},
   "source": [
    "### Create hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4fced82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_units': [64, 32],\n",
       " 'learning_rate': 0.0001,\n",
       " 'batch_size': 512,\n",
       " 'num_epochs': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da02b062-627c-421d-b4c6-e7643b4a9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common import features, datasource_utils\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cee0ed9f-b43c-4489-ac6c-a19e83cc90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow_transform import TFTransformOutput\n",
    "# from absl import logging\n",
    "\n",
    "# def bkm(tf_transform_output: TFTransformOutput\n",
    "#                        ) -> tf.keras.Model:\n",
    "#   \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
    "\n",
    "#   Args:\n",
    "#     tf_transform_output: [TFTransformOutput], the outputs from Transform\n",
    "\n",
    "#   Returns:\n",
    "#     A keras Model.\n",
    "#   \"\"\"\n",
    "#   feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "#   feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "#   inputs = {}\n",
    "#   for key, spec in feature_spec.items():\n",
    "#     if isinstance(spec, tf.io.VarLenFeature):\n",
    "#       inputs[key] = tf.keras.layers.Input(\n",
    "#           shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
    "#     elif isinstance(spec, tf.io.FixedLenFeature):\n",
    "#       # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n",
    "#       # [] for scalars.\n",
    "#       inputs[key] = tf.keras.layers.Input(\n",
    "#           shape=spec.shape or [1], name=key, dtype=spec.dtype)\n",
    "#     else:\n",
    "#       raise ValueError('Spec type is not supported: ', key, spec)\n",
    "\n",
    "#   output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n",
    "#   output = tf.keras.layers.Dense(100, activation='relu')(output)\n",
    "#   output = tf.keras.layers.Dense(70, activation='relu')(output)\n",
    "#   output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
    "#   output = tf.keras.layers.Dense(20, activation='relu')(output)\n",
    "#   output = tf.keras.layers.Dense(1)(output)\n",
    "#   return tf.keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44d000a5-fe17-48c7-8603-92a02f70c2a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Spec type is not supported: ', 'trip_seconds_xf', 'trip_month')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_32749/3684692539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m               shape=[1], name=key, dtype=tf.float32)\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Spec type is not supported: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: ('Spec type is not supported: ', 'trip_seconds_xf', 'trip_month')"
     ]
    }
   ],
   "source": [
    "# bkm(tft_output)\n",
    "# features.FEATURE_NAMES\n",
    "inputs = {}\n",
    "d = features\n",
    "for feature_name in features.FEATURE_NAMES:\n",
    "    name = features.transformed_name(feature_name)\n",
    "    if isinstance(feature_name, tf.io.VarLenFeature):\n",
    "          inputs[name] = tf.keras.layers.Input(\n",
    "              shape=[None], name=key, dtype=tf.float32, sparse=True)\n",
    "    elif isinstance(feature_name, tf.io.FixedLenFeature):\n",
    "      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n",
    "      # [] for scalars.\n",
    "          inputs[name] = tf.keras.layers.Input(\n",
    "              shape=[1], name=key, dtype=tf.float32)\n",
    "    else:\n",
    "          raise ValueError('Spec type is not supported: ', key, feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "894f1b1a-4de2-4202-ab41-b77276bf4344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trip_month_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'trip_day_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'trip_day_of_week_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'trip_hour_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'trip_seconds_xf': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'trip_seconds_xf')>,\n",
       " 'trip_miles_xf': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'trip_seconds_xf')>,\n",
       " 'payment_type_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'pickup_grid_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'dropoff_grid_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>,\n",
       " 'euclidean_xf': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'trip_seconds_xf')>,\n",
       " 'loc_cross_xf': <KerasTensor: type_spec=RaggedTensorSpec(TensorShape([None, 1]), tf.float32, 0, tf.int64) (created by layer 'trip_seconds_xf')>}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model_inp():\n",
    "    inputs = {}\n",
    "    for feature_name in features.FEATURE_NAMES:\n",
    "        name = features.transformed_name(feature_name)\n",
    "        if feature_name in features.NUMERICAL_FEATURE_NAMES:\n",
    "            inputs[name] = keras.layers.Input(name=key, shape=(1,), dtype=tf.float32, ragged=True)\n",
    "        elif feature_name in features.categorical_feature_names():\n",
    "            inputs[name] = keras.layers.Input(name=key, shape=(1,), dtype=tf.float32, ragged=True)\n",
    "        else:\n",
    "            pass\n",
    "    return inputs\n",
    "test_layer = create_model_inp()\n",
    "test_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6bb937f2-0b90-497b-9180-2b7e90efc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common import features\n",
    "\n",
    "def create_model_inp():\n",
    "    inputs = {}\n",
    "    for feature_name in features.FEATURE_NAMES:\n",
    "        name = features.transformed_name(feature_name)\n",
    "        if feature_name in features.NUMERICAL_FEATURE_NAMES:\n",
    "            inputs[name] = keras.layers.Input(name=name, shape=(1,), dtype=tf.float32, ragged=True)\n",
    "        elif feature_name in features.categorical_feature_names():\n",
    "            inputs[name] = keras.layers.Input(name=name, shape=(1,), dtype=tf.int64, ragged=True)\n",
    "        else:\n",
    "            pass\n",
    "    return inputs\n",
    "\n",
    "def _cbc(feature_vocab_sizes, hyperparams):\n",
    "    input_layers = create_model_inp()\n",
    "    layers = []\n",
    "    for key in input_layers:\n",
    "        feature_name = features.original_name(key)\n",
    "        if feature_name in features.EMBEDDING_CATEGORICAL_FEATURES:\n",
    "            vocab_size = feature_vocab_sizes[feature_name]\n",
    "            embedding_size = features.EMBEDDING_CATEGORICAL_FEATURES[feature_name]\n",
    "            embedding_output = keras.layers.Embedding(\n",
    "                input_dim=vocab_size + 1,\n",
    "                output_dim=embedding_size,\n",
    "                name=f\"{key}_embedding\",\n",
    "            )(input_layers[key])\n",
    "            layers.append(embedding_output)\n",
    "        elif feature_name in features.ONEHOT_CATEGORICAL_FEATURE_NAMES:\n",
    "            vocab_size = feature_vocab_sizes[feature_name]\n",
    "            onehot_layer = keras.layers.experimental.preprocessing.CategoryEncoding(\n",
    "                num_tokens=vocab_size + 1,\n",
    "                output_mode=\"binary\",\n",
    "                name=f\"{key}_onehot\",\n",
    "            )(input_layers[key])\n",
    "            layers.append(onehot_layer)\n",
    "        elif feature_name in features.NUMERICAL_FEATURE_NAMES:\n",
    "            numeric_layer = keras.layers.Reshape((-1,), name=f\"{key}_numeric\")(input_layers[key])  # Use Reshape with (-1,) shape\n",
    "            layers.append(numeric_layer)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    reshaped_layers = []\n",
    "    for layer in layers:\n",
    "        if len(layer.shape) > 2:\n",
    "            reshaped_layer = keras.layers.Reshape((-1,))(layer)\n",
    "        else:\n",
    "            reshaped_layer = layer\n",
    "        reshaped_layers.append(reshaped_layer)\n",
    "\n",
    "    joined = keras.layers.Concatenate(name=\"combines_inputs\", axis=-1)(reshaped_layers)\n",
    "    feedforward_output = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(units, activation=\"relu\")\n",
    "            for units in hyperparams[\"hidden_units\"]\n",
    "        ],\n",
    "        name=\"feedforward_network\",\n",
    "    )(joined)\n",
    "    logits = keras.layers.Dense(units=1, name=\"logits\")(feedforward_output)\n",
    "\n",
    "    model = keras.Model(inputs=input_layers, outputs=[logits])\n",
    "    return model\n",
    "\n",
    "def create_binary(tft_output, hyperparams):\n",
    "    feature_vocab_sizes = dict()\n",
    "    for feature_name in features.categorical_feature_names():\n",
    "        feature_vocab_sizes[feature_name] = tft_output.vocabulary_size_by_name(\n",
    "            feature_name\n",
    "        )\n",
    "    return _cbc(feature_vocab_sizes, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d9829-c9e1-4306-a56d-fee795de8d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " trip_month_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_day_xf (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_hour_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dropoff_grid_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " loc_cross_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_month_xf_embedding (Embed  (None, 1, 2)        18          ['trip_month_xf[0][0]']          \n",
      " ding)                                                                                            \n",
      "                                                                                                  \n",
      " trip_day_xf_embedding (Embeddi  (None, 1, 4)        128         ['trip_day_xf[0][0]']            \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf (InputLaye  [(None, 1)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " trip_hour_xf_embedding (Embedd  (None, 1, 3)        75          ['trip_hour_xf[0][0]']           \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " trip_seconds_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " trip_miles_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " payment_type_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " pickup_grid_xf_embedding (Embe  (None, 1, 3)        9           ['pickup_grid_xf[0][0]']         \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " dropoff_grid_xf_embedding (Emb  (None, 1, 3)        12          ['dropoff_grid_xf[0][0]']        \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " euclidean_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " loc_cross_xf_embedding (Embedd  (None, 1, 10)       60          ['loc_cross_xf[0][0]']           \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " reshape_135 (Reshape)          (None, 2)            0           ['trip_month_xf_embedding[0][0]']\n",
      "                                                                                                  \n",
      " reshape_136 (Reshape)          (None, 4)            0           ['trip_day_xf_embedding[0][0]']  \n",
      "                                                                                                  \n",
      " trip_day_of_week_xf_onehot (Ca  (None, 8)           0           ['trip_day_of_week_xf[0][0]']    \n",
      " tegoryEncoding)                                                                                  \n",
      "                                                                                                  \n",
      " reshape_137 (Reshape)          (None, 3)            0           ['trip_hour_xf_embedding[0][0]'] \n",
      "                                                                                                  \n",
      " trip_seconds_xf_numeric (Resha  (None, 1)           0           ['trip_seconds_xf[0][0]']        \n",
      " pe)                                                                                              \n",
      "                                                                                                  \n",
      " trip_miles_xf_numeric (Reshape  (None, 1)           0           ['trip_miles_xf[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " payment_type_xf_onehot (Catego  (None, 7)           0           ['payment_type_xf[0][0]']        \n",
      " ryEncoding)                                                                                      \n",
      "                                                                                                  \n",
      " reshape_138 (Reshape)          (None, 3)            0           ['pickup_grid_xf_embedding[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " reshape_139 (Reshape)          (None, 3)            0           ['dropoff_grid_xf_embedding[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " euclidean_xf_numeric (Reshape)  (None, 1)           0           ['euclidean_xf[0][0]']           \n",
      "                                                                                                  \n",
      " reshape_140 (Reshape)          (None, 10)           0           ['loc_cross_xf_embedding[0][0]'] \n",
      "                                                                                                  \n",
      " combines_inputs (Concatenate)  (None, 43)           0           ['reshape_135[0][0]',            \n",
      "                                                                  'reshape_136[0][0]',            \n",
      "                                                                  'trip_day_of_week_xf_onehot[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'reshape_137[0][0]',            \n",
      "                                                                  'trip_seconds_xf_numeric[0][0]',\n",
      "                                                                  'trip_miles_xf_numeric[0][0]',  \n",
      "                                                                  'payment_type_xf_onehot[0][0]', \n",
      "                                                                  'reshape_138[0][0]',            \n",
      "                                                                  'reshape_139[0][0]',            \n",
      "                                                                  'euclidean_xf_numeric[0][0]',   \n",
      "                                                                  'reshape_140[0][0]']            \n",
      "                                                                                                  \n",
      " feedforward_network (Sequentia  (None, 32)          4896        ['combines_inputs[0][0]']        \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " logits (Dense)                 (None, 1)            33          ['feedforward_network[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,231\n",
      "Trainable params: 5,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classy = create_binary(tft_output, hyperparams)\n",
    "classy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17b65e",
   "metadata": {},
   "source": [
    "### Create and test model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a05b0f63",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_32749/3469005244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_binary_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtft_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlops-with-vertex-ai/src/model_training/model.py\u001b[0m in \u001b[0;36mcreate_binary_classifier\u001b[0;34m(tft_output, hyperparams)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mfeature_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         )\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_create_binary_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vocab_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlops-with-vertex-ai/src/model_training/model.py\u001b[0m in \u001b[0;36m_create_binary_classifier\u001b[0;34m(feature_vocab_sizes, hyperparams)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_binary_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vocab_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0minput_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mfeature_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlops-with-vertex-ai/src/model_training/model.py\u001b[0m in \u001b[0;36mcreate_model_inputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "classifier = model.create_binary_classifier(tft_output, hyperparams)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299ae68-cb36-4fca-9e97-8c24d219d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "test = [(None, 2), (None, 4), (7,), (None, 3), (None, 1), (None, 1), (6,), (None, 3), (None, 3), (None, 1), (None, 10)]\n",
    "test = tuple(chain.from_iterable(test[1]))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d451ef7",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153efc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 5\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(\n",
    "    model=classifier,\n",
    "    data_dir=eval_data_file_pattern,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_metrics(\n",
    "    {\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44268d04",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1ba3e",
   "metadata": {},
   "source": [
    "### Inspect model serving signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8989c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68ecd0",
   "metadata": {},
   "source": [
    "### Test the exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)\n",
    "print(\"Saved model is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serving_tf_example with TF Examples\n",
    "\n",
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '/data-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serving_default with feature dictionary\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": 12,\n",
    "    \"trip_hour\": 6,\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day_of_week\": 4,\n",
    "    \"trip_seconds\": 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec2080",
   "metadata": {},
   "source": [
    "## Start a new Vertex AI experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME)\n",
    "\n",
    "run_id = f\"run-gcp-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade27b7",
   "metadata": {},
   "source": [
    "## 3. Submit a Data Processing Job to Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d903ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 1000000\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    ml_use=ML_USE, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "etl_job_name = f\"etl-{MODEL_DISPLAY_NAME}-{run_id}\"\n",
    "\n",
    "args = {\n",
    "    'job_name': etl_job_name,\n",
    "    'runner': 'DataflowRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a21c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4dfa8",
   "metadata": {},
   "source": [
    "## 4. Submit a Custom Training Job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e998139",
   "metadata": {},
   "source": [
    "### Test the training task locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTIFACTS_DIR} \\\n",
    "    --num-epochs=3 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077e9d4",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')\n",
    "TRAINER_PACKAGE_NAME = f'{MODEL_DISPLAY_NAME}_trainer'\n",
    "print(\"Trainer package upload location:\", TRAINER_PACKAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/__pycache__/\n",
    "!rm -r src/.ipynb_checkpoints/\n",
    "!rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}.tar.gz\n",
    "\n",
    "!mkdir {TRAINER_PACKAGE_NAME}\n",
    "\n",
    "!cp setup.py {TRAINER_PACKAGE_NAME}/\n",
    "!cp -r src {TRAINER_PACKAGE_NAME}/\n",
    "!tar cvf {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}\n",
    "!gzip {TRAINER_PACKAGE_NAME}.tar\n",
    "!gsutil cp {TRAINER_PACKAGE_NAME}.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r {TRAINER_PACKAGE_NAME}\n",
    "!rm -r {TRAINER_PACKAGE_NAME}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3af757",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RUNTIME = 'tf-cpu.2-5'\n",
    "TRAIN_IMAGE = f\"us-docker.pkg.dev/vertex-ai/training/{TRAIN_RUNTIME}:latest\"\n",
    "print(\"Training image:\", TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_units = \"64,64\"\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX + \"/train/*\"}',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX + \"/eval/*\"}',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTIFACTS_DIR}',\n",
    "    f'--num-epochs={num_epochs}',\n",
    "    f'--learning-rate={learning_rate}',\n",
    "    f'--project={PROJECT}',\n",
    "    f'--region={REGION}',\n",
    "    f'--staging-bucket={BUCKET}',\n",
    "    f'--experiment-name={EXPERIMENT_NAME}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_uri = os.path.join(TRAINER_PACKAGE_DIR, f'{TRAINER_PACKAGE_NAME}.tar.gz')\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": 'n1-standard-4',\n",
    "            \"accelerator_count\": 0\n",
    "    },\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": \"src.model_training.task\",\n",
    "            \"args\": trainer_args,\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d6e99",
   "metadata": {},
   "source": [
    "### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a custom training job...\")\n",
    "\n",
    "training_job_display_name = f\"{TRAINER_PACKAGE_NAME}_{run_id}\"\n",
    "\n",
    "training_job = vertex_ai.CustomJob(\n",
    "    display_name=training_job_display_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=EXPERIMENT_RUN_DIR,\n",
    ")\n",
    "\n",
    "training_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2896b59",
   "metadata": {},
   "source": [
    "## 5. Upload exported model to Vertex AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e0c39",
   "metadata": {},
   "source": [
    "### Generate the Explanation metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_config = features.generate_explanation_config()\n",
    "explanation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd0587",
   "metadata": {},
   "source": [
    "### Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-5'\n",
    "SERVING_IMAGE = f\"us-docker.pkg.dev/vertex-ai/prediction/{SERVING_RUNTIME}:latest\"\n",
    "print(\"Serving image:\", SERVING_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "    inputs=explanation_config[\"inputs\"],\n",
    "    outputs=explanation_config[\"outputs\"],\n",
    ")\n",
    "explanation_parameters = vertex_ai.explain.ExplanationParameters(\n",
    "    explanation_config[\"params\"]\n",
    ")\n",
    "\n",
    "vertex_model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=SERVING_IMAGE,\n",
    "    parameters_schema_uri=None,\n",
    "    instance_schema_uri=None,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters,\n",
    "    labels={\n",
    "        'dataset_name': DATASET_DISPLAY_NAME,\n",
    "        'experiment': run_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51756dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model.gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa55220",
   "metadata": {},
   "source": [
    "## 6. Extract experiment run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07808f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = vertex_ai.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/vertex-ai/locations{REGION}/experiments/{EXPERIMENT_NAME}/metrics?project={PROJECT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96618f6",
   "metadata": {},
   "source": [
    "## 7. Submit a Hyperparameter Tuning Job to Vertex AI\n",
    "\n",
    "For more information about configuring a hyperparameter study, refer to [Vertex AI Hyperparameter job configuration](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b87cd",
   "metadata": {},
   "source": [
    "### Configure a hyperparameter job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_spec = {\n",
    "    'ACCURACY': 'maximize'\n",
    "}\n",
    "\n",
    "parameter_spec = {\n",
    "    'learning-rate': hp_tuning.DoubleParameterSpec(min=0.0001, max=0.01, scale='log'),\n",
    "    'hidden-units': hp_tuning.CategoricalParameterSpec(values=[\"32,32\", \"64,64\", \"128,128\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2454dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_display_name = f\"hpt_{TRAINER_PACKAGE_NAME}_{run_id}\"\n",
    "\n",
    "hp_tuning_job = vertex_ai.HyperparameterTuningJob(\n",
    "    display_name=tuning_job_display_name,\n",
    "    custom_job=training_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=4,\n",
    "    parallel_trial_count=2,\n",
    "    search_algorithm=None # Bayesian optimization.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4ee22",
   "metadata": {},
   "source": [
    "### Submit the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a hyperparameter tunning job...\")\n",
    "\n",
    "hp_tuning_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd20da",
   "metadata": {},
   "source": [
    "### Retrieve trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8352e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tuning_job.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311922be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = sorted(\n",
    "    hp_tuning_job.trials, \n",
    "    key=lambda trial: trial.final_measurement.metrics[0].value, \n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "print(\"Best trial ID:\", best_trial.id)\n",
    "print(\"Validation Accuracy:\", best_trial.final_measurement.metrics[0].value)\n",
    "print(\"Hyperparameter Values:\")\n",
    "for parameter in best_trial.parameters:\n",
    "    print(f\" - {parameter.parameter_id}:{parameter.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88555c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
